# Low-Rank Adaptation (LoRA)
> It's used to make LLMs work better for specific tasks without redoing all the training.
- LoRA adds smaller trainable parts to each layer of the pretrained model, instead of changing everything.
- The original model remains the same, which saves time and resources.
